{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nstart = time.time()","execution_count":1,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"875b42ec5baee5274279d8a7b7a72159f3a586de","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nnp.random.seed(42)\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os # accessing directory structure\nos.listdir('../input/fashion-dataset/fashion-dataset/')","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"['images.csv', 'images', 'styles.csv', 'styles']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## params\npath = '../input/fashion-dataset/fashion-dataset/'\nimg_size = 128\nbatch_size = 64","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"f92cc76567188bdfe9dfa0d720d163f2702ab4fb","trusted":true},"cell_type":"code","source":"df = pd.read_csv(path+\"styles.csv\", error_bad_lines=False)\ndf['image'] = df.id.apply(str) + '.jpg'\ndf.head()","execution_count":4,"outputs":[{"output_type":"stream","text":"b'Skipping line 6044: expected 10 fields, saw 11\\nSkipping line 6569: expected 10 fields, saw 11\\nSkipping line 7399: expected 10 fields, saw 11\\nSkipping line 7939: expected 10 fields, saw 11\\nSkipping line 9026: expected 10 fields, saw 11\\nSkipping line 10264: expected 10 fields, saw 11\\nSkipping line 10427: expected 10 fields, saw 11\\nSkipping line 10905: expected 10 fields, saw 11\\nSkipping line 11373: expected 10 fields, saw 11\\nSkipping line 11945: expected 10 fields, saw 11\\nSkipping line 14112: expected 10 fields, saw 11\\nSkipping line 14532: expected 10 fields, saw 11\\nSkipping line 15076: expected 10 fields, saw 12\\nSkipping line 29906: expected 10 fields, saw 11\\nSkipping line 31625: expected 10 fields, saw 11\\nSkipping line 33020: expected 10 fields, saw 11\\nSkipping line 35748: expected 10 fields, saw 11\\nSkipping line 35962: expected 10 fields, saw 11\\nSkipping line 37770: expected 10 fields, saw 11\\nSkipping line 38105: expected 10 fields, saw 11\\nSkipping line 38275: expected 10 fields, saw 11\\nSkipping line 38404: expected 10 fields, saw 12\\n'\n","name":"stderr"},{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"      id    ...          image\n0  15970    ...      15970.jpg\n1  39386    ...      39386.jpg\n2  59263    ...      59263.jpg\n3  21379    ...      21379.jpg\n4  53759    ...      53759.jpg\n\n[5 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>gender</th>\n      <th>masterCategory</th>\n      <th>subCategory</th>\n      <th>articleType</th>\n      <th>baseColour</th>\n      <th>season</th>\n      <th>year</th>\n      <th>usage</th>\n      <th>productDisplayName</th>\n      <th>image</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15970</td>\n      <td>Men</td>\n      <td>Apparel</td>\n      <td>Topwear</td>\n      <td>Shirts</td>\n      <td>Navy Blue</td>\n      <td>Fall</td>\n      <td>2011.0</td>\n      <td>Casual</td>\n      <td>Turtle Check Men Navy Blue Shirt</td>\n      <td>15970.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>39386</td>\n      <td>Men</td>\n      <td>Apparel</td>\n      <td>Bottomwear</td>\n      <td>Jeans</td>\n      <td>Blue</td>\n      <td>Summer</td>\n      <td>2012.0</td>\n      <td>Casual</td>\n      <td>Peter England Men Party Blue Jeans</td>\n      <td>39386.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59263</td>\n      <td>Women</td>\n      <td>Accessories</td>\n      <td>Watches</td>\n      <td>Watches</td>\n      <td>Silver</td>\n      <td>Winter</td>\n      <td>2016.0</td>\n      <td>Casual</td>\n      <td>Titan Women Silver Watch</td>\n      <td>59263.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>21379</td>\n      <td>Men</td>\n      <td>Apparel</td>\n      <td>Bottomwear</td>\n      <td>Track Pants</td>\n      <td>Black</td>\n      <td>Fall</td>\n      <td>2011.0</td>\n      <td>Casual</td>\n      <td>Manchester United Men Solid Black Track Pants</td>\n      <td>21379.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>53759</td>\n      <td>Men</td>\n      <td>Apparel</td>\n      <td>Topwear</td>\n      <td>Tshirts</td>\n      <td>Grey</td>\n      <td>Summer</td>\n      <td>2012.0</td>\n      <td>Casual</td>\n      <td>Puma Men Grey T-shirt</td>\n      <td>53759.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''sns.countplot(df.subCategory)\nplt.title('Unique subCategory '+str(df.subCategory.nunique()))\nplt.xticks(rotation = 90)'''","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"\"sns.countplot(df.subCategory)\\nplt.title('Unique subCategory '+str(df.subCategory.nunique()))\\nplt.xticks(rotation = 90)\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('before:',df.shape)\ntop10_subcats = df.subCategory.value_counts().sort_values(ascending=False).reset_index().head(10).iloc[:,0].tolist()\ndf = df[df.subCategory.isin(top10_subcats)]\nprint('after:',df.shape)","execution_count":6,"outputs":[{"output_type":"stream","text":"before: (44424, 11)\nafter: (36970, 11)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub = df[['subCategory','image']]\n# df_sub['image'] = path + 'images/' +df_sub.image\ndf_sub.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"  subCategory      image\n0     Topwear  15970.jpg\n1  Bottomwear  39386.jpg\n2     Watches  59263.jpg\n3  Bottomwear  21379.jpg\n4     Topwear  53759.jpg","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subCategory</th>\n      <th>image</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Topwear</td>\n      <td>15970.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Bottomwear</td>\n      <td>39386.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Watches</td>\n      <td>59263.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Bottomwear</td>\n      <td>21379.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Topwear</td>\n      <td>53759.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, KFold\n\nX_train, X_test, y_train, y_test = train_test_split(df_sub, df_sub.image, test_size = 0.2, random_state=42)\nX_train.shape, X_test.shape","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"((29576, 2), (7394, 2))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1/255.,\n    # featurewise_center=True,\n    # featurewise_std_normalization=True,\n    rotation_range=20,\n    zoom_range=0.2,\n    # width_shift_range=0.2,\n    # height_shift_range=0.2,\n    horizontal_flip=True)\n\n\ntrain_flow = train_datagen.flow_from_dataframe(X_train, directory=path+'/images/', \n                                    x_col='image', y_col='subCategory', \n                                    target_size=(img_size, img_size), \n                                    color_mode='grayscale', \n                                    classes=top10_subcats, \n                                    class_mode='categorical', \n                                    batch_size=batch_size, \n                                    shuffle=False, seed=42, \n                                    interpolation='nearest', drop_duplicates=False)\n\ntest_datagen = ImageDataGenerator(\n    rescale=1/255.,\n    # featurewise_center=True,\n    # featurewise_std_normalization=True,\n    # rotation_range=20,\n    # width_shift_range=0.2,\n    # height_shift_range=0.2,\n    # horizontal_flip=True\n)\n\n\ntest_flow = test_datagen.flow_from_dataframe(X_test, directory=path+'/images/', \n                                    x_col='image', y_col='subCategory', \n                                    target_size=(img_size, img_size), \n                                    color_mode='grayscale', \n                                    classes=top10_subcats, \n                                    class_mode='categorical', \n                                    batch_size=batch_size, \n                                    shuffle=False, seed=42)","execution_count":9,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"Found 29572 images belonging to 10 classes.\nFound 7393 images belonging to 10 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"STEP_SIZE_TRAIN=(train_flow.n//train_flow.batch_size) + 1\nSTEP_SIZE_VALID=(test_flow.n//test_flow.batch_size) + 1\nSTEP_SIZE_TRAIN, STEP_SIZE_VALID","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"(463, 116)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.mobilenet_v2 import MobileNetV2\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization, Conv2D, MaxPooling2D, Input, Flatten, Concatenate\n\n'''arch = MobileNetV2(input_shape=(img_size, img_size, 3), \n                   include_top=False, \n                   weights=None, \n                   pooling=None)\ndense = arch.output\ndense = GlobalAveragePooling2D()(dense)\ndense = Dense(512, activation='relu')(dense)\ndense = Dense(10, activation='softmax')(dense)'''\n\ninputs = Input(shape=(img_size, img_size, 1))\nbn = BatchNormalization()(inputs)\n\nconv = Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n                 activation='relu')(bn)\nconv = Conv2D(32, kernel_size=(3, 3), strides=(1, 1),\n                 activation='relu')(conv)\nconv = MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(conv)\n\nconv = Conv2D(64, kernel_size=(3, 3), strides=(1, 1),\n                 activation='relu')(conv)\nconv = Conv2D(64, kernel_size=(3, 3), strides=(1, 1),\n                 activation='relu')(conv)\nconv = MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(conv)\n\nconv = Conv2D(128, kernel_size=(3, 3), strides=(1, 1),\n                 activation='relu')(conv)\nconv = Conv2D(128, kernel_size=(3, 3), strides=(1, 1),\n                 activation='relu')(conv)\nconv = MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(conv)\n\nconv = Conv2D(256, kernel_size=(3, 3), strides=(1, 1),\n                 activation='relu')(conv)\nconv = Conv2D(256, kernel_size=(3, 3), strides=(1, 1),\n                 activation='relu')(conv)\nconv = MaxPooling2D(pool_size=(2, 2), strides=(1, 1))(conv)\n\npool = GlobalAveragePooling2D()(conv)\n\ndense = Dense(256, activation='relu')(pool)\ndense = Dense(10, activation='softmax')(dense)\n\ncnn = Model(inputs=inputs, outputs=dense)\n\ncnn.summary()\n\ncnn.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='sgd')","execution_count":11,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 128, 128, 1)       0         \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 128, 128, 1)       4         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 126, 126, 32)      320       \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 124, 124, 32)      9248      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 123, 123, 32)      0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 121, 121, 64)      18496     \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 119, 119, 64)      36928     \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 118, 118, 64)      0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 116, 116, 128)     73856     \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 114, 114, 128)     147584    \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 113, 113, 128)     0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 111, 111, 256)     295168    \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 109, 109, 256)     590080    \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 108, 108, 256)     0         \n_________________________________________________________________\nglobal_average_pooling2d_1 ( (None, 256)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 256)               65792     \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                2570      \n=================================================================\nTotal params: 1,240,046\nTrainable params: 1,240,044\nNon-trainable params: 2\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, CSVLogger\n\nes = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\nlog = CSVLogger('./train_performance.csv', separator=',', append=True)\n\ncbs = [#es, \n       log\n      ]","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_history = cnn.fit_generator(train_flow, epochs=50, steps_per_epoch=STEP_SIZE_TRAIN, \n                                    validation_data=test_flow, validation_steps = STEP_SIZE_VALID, \n                                    callbacks = cbs)","execution_count":null,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/50\n","name":"stdout"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(train_history.history['loss'])\nplt.plot(train_history.history['val_loss'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(train_history.history['acc'])\nplt.plot(train_history.history['val_acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"preds = cnn.predict_generator(test_flow, steps=STEP_SIZE_VALID, verbose = 1)\npreds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"subcat_lkup = dict((v,k) for k,v in test_flow.class_indices.items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"preds_i = np.argmax(preds, axis=1)\npreds_i.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"preds_fin = preds_i.tolist()\npreds_fin = [subcat_lkup.get(i) for i in preds_fin]\nacts_fin = [subcat_lkup.get(i) for i in test_flow.classes]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nplt.imshow(confusion_matrix(acts_fin, preds_fin))\nplt.xticks(ticks=np.arange(10), labels=[v for k,v in subcat_lkup.items()], rotation=90)\nplt.yticks(ticks=np.arange(10), labels=[v for k,v in subcat_lkup.items()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}